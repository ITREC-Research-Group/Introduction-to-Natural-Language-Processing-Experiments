{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用CNN进行文本分类 \n",
    "\n",
    "<img src='img/textcnn.jfif' width=500>\n",
    "\n",
    "reference:\n",
    "- https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "- https://github.com/649453932/Chinese-Text-Classification-Pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=16, dropout=0.2, embedding_dim=200, filter_sizes=[1, 2, 3], hidden_dim=200, learning_rate=0.004, max_length=2000, model_save_path='data/save_model/textcnn.path', num_filters=200, num_train_epochs=20, seed=42)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "hparams = argparse.Namespace(**{\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.004,\n",
    "    # 'max_grad_norm': 1.,\n",
    "    'max_length': 2000,\n",
    "    'dropout': 0.2,\n",
    "    'embedding_dim': 200,\n",
    "    'hidden_dim': 200,\n",
    "    'seed': 42,\n",
    "    'num_filters': 200,\n",
    "    'filter_sizes': [1, 2, 3],\n",
    "    'num_train_epochs': 20,\n",
    "    'model_save_path': 'data/save_model/textcnn.path',\n",
    "})\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1600\n",
      "test: 400\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "random.seed(hparams.seed)\n",
    "\n",
    "\n",
    "def load_movie_reviews():\n",
    "    pos_ids = movie_reviews.fileids('pos')\n",
    "    neg_ids = movie_reviews.fileids('neg')\n",
    "\n",
    "    all_reviews = []\n",
    "    for pids in pos_ids:\n",
    "        all_reviews.append((movie_reviews.raw(pids), 'positive'))\n",
    "    \n",
    "    for nids in neg_ids:\n",
    "        all_reviews.append((movie_reviews.raw(nids), 'negative'))\n",
    "\n",
    "    random.shuffle(all_reviews)\n",
    "    train_reviews = all_reviews[:1600]\n",
    "    test_reviews = all_reviews[1600:]\n",
    "\n",
    "    return train_reviews, test_reviews\n",
    "\n",
    "train_reviews, test_reviews = load_movie_reviews()\n",
    "print('train:', len(train_reviews))\n",
    "print('test:', len(test_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "train_reviews_tokenized = []\n",
    "train_labels = []\n",
    "\n",
    "for review, label in train_reviews:\n",
    "    label = 0 if label == 'negative' else 1\n",
    "    tokenized = word_tokenize(review)\n",
    "\n",
    "    train_labels.append(label)\n",
    "    train_reviews_tokenized.append(tokenized)\n",
    "\n",
    "\n",
    "test_reviews_tokenized = []\n",
    "test_labels = []\n",
    "\n",
    "for review, label in test_reviews:\n",
    "    label = 0 if label == 'negative' else 1\n",
    "    tokenized = word_tokenize(review)\n",
    "\n",
    "    test_labels.append(label)\n",
    "    test_reviews_tokenized.append(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词表、将单词变成id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42013\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "for review in train_reviews_tokenized:# + test_reviews_tokenized:\n",
    "    counter.update(review)\n",
    "\n",
    "vocab = vocab(counter, min_freq=1, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "# vocab = Vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n",
    "\n",
    "hparams.vocab_size = len(vocab)\n",
    "hparams.pad_id = vocab['<pad>']\n",
    "hparams.num_classes = 2\n",
    "\n",
    "print(hparams.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab['<unk>'])\n",
    "train_reviews_ids = [vocab.lookup_indices(review) for review in train_reviews_tokenized]\n",
    "test_reviews_ids = [vocab.lookup_indices(review) for review in test_reviews_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据打包为dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.reviews[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "\n",
    "def collate_to_max_length(batch):\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    for X, y in batch:\n",
    "        if len(X) >= hparams.max_length:\n",
    "            X = X[:hparams.max_length]\n",
    "        else:\n",
    "            X = X + [hparams.pad_id] * (hparams.max_length-len(X))\n",
    "\n",
    "        X_batch.append(X)\n",
    "        y_batch.append(y)\n",
    "\n",
    "    return torch.tensor(X_batch), torch.tensor(y_batch)\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_reviews_ids, train_labels)\n",
    "test_dataset = TextDataset(test_reviews_ids, test_labels)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=hparams.batch_size, \n",
    "    collate_fn=collate_to_max_length, \n",
    "    shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=hparams.batch_size,\n",
    "    collate_fn=collate_to_max_length,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams    \n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            hparams.vocab_size, \n",
    "            hparams.embedding_dim, \n",
    "            padding_idx=hparams.pad_id)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, hparams.num_filters, (k, hparams.embedding_dim))\n",
    "            for k in hparams.filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    "        hidden_size = hparams.num_filters * len(hparams.filter_sizes)\n",
    "        self.classifier = nn.Linear(hidden_size, hparams.num_classes)\n",
    "            \n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, w in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                # w.data.xavier_normal_()\n",
    "                nn.init.xavier_normal_(w)\n",
    "            elif 'bias' in name:\n",
    "                w.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B, L, embedding_dim]\n",
    "        embed = self.embedding(x)\n",
    "        # [B, 1, L, embedding_dim]\n",
    "        embed = embed.unsqueeze(1)\n",
    "        \n",
    "        # [(B, num_filters), ...] => [(B, num_filters*len(filter_sizes))]\n",
    "        hidden = torch.cat([self.conv_and_pool(embed, conv) for conv in self.convs], dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.classifier(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        # (B, 1, L, embedding_dim) => (B, 1, L, 1, num_filters)\n",
    "        # (B, 1, L, 1, num_filters) => (B, 1, L, num_filters)\n",
    "        x = F.relu(conv(x).squeeze(3))\n",
    "        # (B, 1, L, num_filters) => (B, 1, num_filters)\n",
    "        # (B, 1, num_filters) => (B, num_filters)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.load_vectors('glove.6B.200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(hparams)\n",
    "\n",
    "# model.embedding.weight.data.copy_(vocab.vectors)\n",
    "# model.embedding.weight.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=hparams.learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95**epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, loss_func, optimizer, epoch_idx, hparams):\n",
    "    model.train()\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    pbar.set_description(f'Epoch {epoch_idx}')\n",
    "\n",
    "    for X, y in pbar:\n",
    "        if torch.cuda.is_available():\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)  # (B, 2)\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, loss_func):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0.\n",
    "    correct_num = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader)\n",
    "        pbar.set_description('Valid')\n",
    "        for X, y in pbar:\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            output = model(X)\n",
    "            \n",
    "            loss = loss_func(output, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            correct_num = correct_num + (output.argmax(1) == y).float().sum().item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct_num / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.52it/s, loss=2.31]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 95.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 2.6607, accuracy: 0.5050, LR: [0.0038]     \n",
      "\r",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 58.40it/s, loss=0.765]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 95.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] loss: 0.8244, accuracy: 0.6925, LR: [0.00361]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.93it/s, loss=0.442]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.5675, accuracy: 0.7850, LR: [0.0034295]     \n",
      "\r",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.51it/s, loss=0.345]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.8155, accuracy: 0.7300, LR: [0.0032580249999999995]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.93it/s, loss=0.701]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] loss: 0.5656, accuracy: 0.7875, LR: [0.003095123749999999]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.68it/s, loss=0.0219]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.9408, accuracy: 0.7500, LR: [0.0029403675624999994]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.93it/s, loss=0.333]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.6598, accuracy: 0.7850, LR: [0.002793349184374999]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.86it/s, loss=0.471]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.7276, accuracy: 0.8000, LR: [0.002653681725156249]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|███████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.90it/s, loss=0.989]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 1.4216, accuracy: 0.7000, LR: [0.0025209976388984364]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|█████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.97it/s, loss=0.0247]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] loss: 0.5436, accuracy: 0.8350, LR: [0.0023949477569535148]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.97it/s, loss=0.00733]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.6729, accuracy: 0.8025, LR: [0.0022752003691058386]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|█████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.90it/s, loss=0.0124]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 97.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] loss: 0.5416, accuracy: 0.8325, LR: [0.0021614403506505465]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.90it/s, loss=0.00684]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.5546, accuracy: 0.8400, LR: [0.002053368333118019]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.86it/s, loss=0.118]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 94.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.5583, accuracy: 0.8350, LR: [0.0019506999164621182]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|█████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 60.08it/s, loss=0.0949]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 91.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.6302, accuracy: 0.8350, LR: [0.001853164920639012]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.72it/s, loss=0.277]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 91.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 1.0945, accuracy: 0.7650, LR: [0.0017605066746070614]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|█████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.83it/s, loss=0.0481]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.5800, accuracy: 0.8350, LR: [0.0016724813408767083]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|█████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.86it/s, loss=0.0172]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 97.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.5524, accuracy: 0.8525, LR: [0.0015888572738328728]     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|█████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.82it/s, loss=0.0102]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 95.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] loss: 0.5377, accuracy: 0.8500, LR: [0.001509414410141229]     \n",
      "save model to data/save_model/textcnn.path\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|███████████████████████████████████████████████████████| 100/100 [00:01<00:00, 59.65it/s, loss=0.000122]\n",
      "Valid: 100%|███████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 96.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Validation] loss: 0.6067, accuracy: 0.8250, LR: [0.0014339436896341675]     \n",
      "accuracy_at_lowest_loss: 0.85, best_accuracy: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "accuracy_at_lowest_loss = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch_idx in range(hparams.num_train_epochs):\n",
    "    train(model, train_dataloader, loss_func, optimizer, epoch_idx+1, hparams)\n",
    "    scheduler.step()\n",
    "    val_loss, accuracy = evaluate(model, test_dataloader, loss_func)\n",
    "    best_accuracy = max(best_accuracy, accuracy)\n",
    "    print(f'\\r[Validation] loss: {val_loss:.4f}, accuracy: {accuracy:.4f}, LR: {scheduler.get_last_lr()}     ')\n",
    "\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), hparams.model_save_path)\n",
    "        print(f'\\rsave model to {hparams.model_save_path}\\n\\n')\n",
    "        best_val_loss = val_loss\n",
    "        accuracy_at_lowest_loss = accuracy\n",
    "\n",
    "print(f'accuracy_at_lowest_loss: {accuracy_at_lowest_loss}, best_accuracy: {best_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "拓展\n",
    "\n",
    "- 如何理解textcnn中的卷积核和pooling层\n",
    "- 如何确定卷积核的大小，调参？\n",
    "    - RCNN[1]\n",
    "\n",
    "[1] Lai, Siwei, et al. \"Recurrent convolutional neural networks for text classification.\" AAAI2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
